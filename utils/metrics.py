
import numpy as np
import sqlvalidator as sqlvalidator
from constants import GPT4_ENGINE_32k
from numpy.linalg import norm
from retry import retry


def is_sql_valid(sql_string):
    """
    Validation contains:

    not using a missing column
    existing functions
    correct aggregations
    schemaless (not assume that table names and columns in those exist)
    types correctness in functions
    (only on SELECT-statements)
    """
    # Check syntax passes if the string is empty, but we want this to be flagged as invalid SQL
    if sql_string == "":
        return False, "Empty sql query"
    sql_query = sqlvalidator.parse(sql_string)
    try:
        sql_query.is_valid()
        return True, "No SQL errors detected."
    except:
        return False, sql_query.errors


# TODO (Meghna needed) modify with the canned response during the retires period
def is_message_exception(message):
    is_not_oops_message = int("Oops, something went wrong" not in message)
    is_not_nba_beta_message = int(
        "One second, let me check the answer again" not in message
    )
    return is_not_oops_message and is_not_nba_beta_message


def is_sql_execution_empty(execution_result):
    if execution_result == [] or execution_result == str([]):
        return True
    else:
        return False


def end_to_end_groundedness(generated_query, query_execution_result, answer, client):

    system_prompt = """You are an AI assistant. You will be given the definition of an evaluation metric for assessing the quality of an answer in a government stats  question-answering task. Your job is to compute an accurate evaluation score using the provided evaluation metric.

    You will be presented with a CONTEXT and an ANSWER about that CONTEXT. You need to decide whether the ANSWER is entailed by the CONTEXT by choosing one of the following rating:

    10: The ANSWER follows logically from the information contained in the CONTEXT.
    0: The ANSWER is logically false from the information contained in the CONTEXT.
    an integer score between 0 and 10 and if such integer score does not exists, use 0: It is not possible to determine whether the ANSWER is true or false without further information.

    Read the passage of information thoroughly and select the correct answer from the three answer labels. Read the CONTEXT thoroughly to ensure you know what the CONTEXT entails.

    Note the ANSWER is generated by a computer system, it can contain certain symbols, which should not be a negative factor in the evaluation.

    Independent Examples:
    Example Task #1 Input:
    "CONTEXT": 
    Query: "SELECT industry_name, SUM(value) as total_job_openings FROM bls_job_turnover WHERE state = 'Washington' AND year = 2020 GROUP BY industry_name ORDER BY total_job_openings DESC LIMIT 1 -- selects the industry with the most job openings in Washington in 2020"
    Query results: Total nonfarm
    
    "ANSWER": "The industry with the most job openings in Washington in 2020 was 'Total nonfarm'."

    Example Task #1 Output:
    10

    Example Task #2 Input:
    "CONTEXT": 
    Query: "SELECT area_name, MAX(value) - MIN(value) as increase_in_unemployment_rate
    FROM bls_local_area_unemployment_statistics
    WHERE year >= 2010 AND measure = 'unemployment rate' AND area_type = 'cities and towns above 25,000 population'
    GROUP BY area_name
    ORDER BY increase_in_unemployment_rate DESC
    LIMIT 5";
    Query results: San Luis City, AZ; Atlantic City city, NJ; Flint city, MI; Mount Morris township, MI; and Detroit city, MI

    "ANSWER": "The 5 cities where the unemployment rate has increased the most since 2010 are San Luis city, AZ; Atlantic City city, NJ; Flint city, MI; Mount Morris township, MI; and Detroit city, MI."

    Example Task #2 Output:
    10

    Example Task #3 Input:
    "CONTEXT": 
    Query: SELECT SUM(value) AS total_job_openings
    FROM bls_job_turnover
    WHERE state = 'Washington' AND year = 2020
    LIMIT 10;
    Query results: 15773441, 18150000
    
    "ANSWER": "The GDP of Washington state in 2020 was approximately $9,773,441,000,000.0 and the number of job openings was approximately 18,150,000."

    Example Task #3 Output:
    0
    """

    user_prompt = f"""
    Actual Task Input:
    "CONTEXT": Query: {generated_query}
    Query results:{query_execution_result}, "ANSWER": {answer}

    Actual Task Output:"""
    try:
        response = client.chat.completions.create(
            model=GPT4_ENGINE_32k,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
        )
    except Exception as e:
        return np.nan

    score = response.choices[0].message.content

    try:
        score = int(score) / 10
    except Exception:
        score = np.nan
    return score


def end_to_end_relevance(question, answer, client):
    prompt = f"""Is the answer relevant and related to the question?
If the answer is yes, output 1.
Respond 0 otherwise.

Question: {question}
Answer: {answer}
Score:
"""
    try:
        response = client.chat.completions.create(
            # TODO: remove hardcoded model, swap for constant
            model="gpt-4-32k",
            messages=[{"role": "system", "content": prompt}],
        )
    except Exception as e:
        return np.nan

    score = response.choices[0].message.content

    try:
        score = int(score)
    except Exception:
        score = np.nan
    return score


def gpt_sql_critic_score(question, generated_query, query_execution_result, client):
    prompt = f"""Your job is to evaluate the nl-to-sql engine of the USA-Facts app.

The app goes through several steps:
1) The user question is sent to a Natural Language to SQL converter along with information about the database schema.
2) The generated SQL query is executed and the output is logged.

Your job is to determine if the generated SQL correctly translates to the original question.

User question: {question}

Generated PostgreSQL: {generated_query}

Executed PostgreSQL result: {query_execution_result}

Given the information above, give a numeric score of 0 to the Generated SQL if it doesn't correctly handle the User question, and give a numeric score of 1 if the Generated SQL query correctly handles the User question.
If the SQL query yields an error, give a numeric score of 0.
If the SQL query doesn't error out, but doesn't correctly handle the User question, give a numeric score of 0.
If the SQL execution results are empty, most likely it didn't handle the User's question. Think hard if you want to give it a 1 or a 0.
Score: """
    try:
        response = client.chat.completions.create(
            # TODO: remove hardcoded model, swap for constant
            model="gpt-4-32k",
            messages=[{"role": "system", "content": prompt}],
        )
    except Exception as e:
        return np.nan

    score = response.choices[0].message.content

    try:
        score = int(score)
    except Exception:
        score = np.nan
    return score


@retry(delay=2, backoff=2, max_delay=10, tries=30)
def get_embedding(text, client):
    try:
        if not text:
            raise ValueError("The sentence must not be empty or None")

        return (
            client.embeddings.create(input=[text], model="text-embedding-ada-002")
            .data[0]
            .embedding
        )

    except Exception as e:
        print(f"An error occurred while getting embedding: {e}")
        return None


def evaluate_similarity(target, generated_sentence, client):
    target_embedding = get_embedding(str(target), client)
    generated_embedding = get_embedding(str(generated_sentence), client)
    if target_embedding and generated_embedding:
        similarity_score = np.dot(target_embedding, generated_embedding) / (
            norm(target_embedding) * norm(generated_embedding)
        )
    else:
        similarity_score = np.nan
    return similarity_score


def calculate_cosine(target_embedding, generated_embedding):
    if target_embedding and generated_embedding:
        similarity_score = np.dot(target_embedding, generated_embedding) / (
            norm(target_embedding) * norm(generated_embedding)
        )
    else:
        similarity_score = np.nan
    return similarity_score
